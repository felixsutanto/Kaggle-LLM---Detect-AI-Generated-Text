{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5392b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:15.473979Z",
     "iopub.status.busy": "2025-07-18T15:57:15.473245Z",
     "iopub.status.idle": "2025-07-18T15:57:21.900867Z",
     "shell.execute_reply": "2025-07-18T15:57:21.900206Z"
    },
    "papermill": {
     "duration": 6.433056,
     "end_time": "2025-07-18T15:57:21.902260",
     "exception": false,
     "start_time": "2025-07-18T15:57:15.469204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4bc87d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:21.909785Z",
     "iopub.status.busy": "2025-07-18T15:57:21.908967Z",
     "iopub.status.idle": "2025-07-18T15:57:24.985281Z",
     "shell.execute_reply": "2025-07-18T15:57:24.984659Z"
    },
    "papermill": {
     "duration": 3.080798,
     "end_time": "2025-07-18T15:57:24.986671",
     "exception": false,
     "start_time": "2025-07-18T15:57:21.905873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6526e801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:24.992633Z",
     "iopub.status.busy": "2025-07-18T15:57:24.992133Z",
     "iopub.status.idle": "2025-07-18T15:57:24.999532Z",
     "shell.execute_reply": "2025-07-18T15:57:24.998872Z"
    },
    "papermill": {
     "duration": 0.011335,
     "end_time": "2025-07-18T15:57:25.000637",
     "exception": false,
     "start_time": "2025-07-18T15:57:24.989302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9757b1a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.006028Z",
     "iopub.status.busy": "2025-07-18T15:57:25.005827Z",
     "iopub.status.idle": "2025-07-18T15:57:25.058360Z",
     "shell.execute_reply": "2025-07-18T15:57:25.057864Z"
    },
    "papermill": {
     "duration": 0.056324,
     "end_time": "2025-07-18T15:57:25.059341",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.003017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceConfig:\n",
    "    \"\"\"Configuration for inference phase\"\"\"\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    HAS_GPU = torch.cuda.is_available()\n",
    "    DEVICE = torch.device('cuda' if HAS_GPU else 'cpu')\n",
    "    \n",
    "    # Model directory\n",
    "    MODEL_SAVE_DIR = \"/kaggle/input/k/modelbuilderpro/llm-detect-ai-generated-text/saved_models/\"\n",
    "    \n",
    "    # Inference parameters\n",
    "    BATCH_SIZE = 16 if HAS_GPU else 8\n",
    "    MAX_LENGTH = 512\n",
    "    DATALOADER_WORKERS = 4 if HAS_GPU else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86853bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.065260Z",
     "iopub.status.busy": "2025-07-18T15:57:25.064839Z",
     "iopub.status.idle": "2025-07-18T15:57:25.070193Z",
     "shell.execute_reply": "2025-07-18T15:57:25.069654Z"
    },
    "papermill": {
     "duration": 0.00934,
     "end_time": "2025-07-18T15:57:25.071210",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.061870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceTextDataset(Dataset):\n",
    "    \"\"\"Dataset for inference\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            text = str(self.texts[idx])\n",
    "            text = re.sub(r'\\s+', ' ', text.strip())\n",
    "            \n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            return {\n",
    "                'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'attention_mask': torch.zeros(self.max_length, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5383cf36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.076533Z",
     "iopub.status.busy": "2025-07-18T15:57:25.076352Z",
     "iopub.status.idle": "2025-07-18T15:57:25.084137Z",
     "shell.execute_reply": "2025-07-18T15:57:25.083518Z"
    },
    "papermill": {
     "duration": 0.011729,
     "end_time": "2025-07-18T15:57:25.085240",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.073511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeBERTaInference:\n",
    "    \"\"\"Load and run inference with trained DeBERTa model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.config = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the saved DeBERTa model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading DeBERTa model from {self.model_path}\")\n",
    "            \n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            \n",
    "            # Load model\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "            self.model = self.model.to(InferenceConfig.DEVICE)\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Load training info if available\n",
    "            info_path = os.path.join(self.model_path, 'training_info.pkl')\n",
    "            if os.path.exists(info_path):\n",
    "                with open(info_path, 'rb') as f:\n",
    "                    self.config = pickle.load(f)\n",
    "                print(f\"Model trained for {self.config.get('config', {}).get('epochs', 'unknown')} epochs\")\n",
    "            \n",
    "            print(\"DeBERTa model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DeBERTa model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"Make predictions on new texts\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        \n",
    "        print(f\"Making DeBERTa predictions on {len(texts)} samples...\")\n",
    "        \n",
    "        try:\n",
    "            predictions = []\n",
    "            dataset = InferenceTextDataset(texts, self.tokenizer, InferenceConfig.MAX_LENGTH)\n",
    "            dataloader = DataLoader(\n",
    "                dataset, \n",
    "                batch_size=InferenceConfig.BATCH_SIZE, \n",
    "                shuffle=False, \n",
    "                num_workers=InferenceConfig.DATALOADER_WORKERS\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in dataloader:\n",
    "                    input_ids = batch['input_ids'].to(InferenceConfig.DEVICE)\n",
    "                    attention_mask = batch['attention_mask'].to(InferenceConfig.DEVICE)\n",
    "                    \n",
    "                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    probs = F.softmax(outputs.logits, dim=-1)\n",
    "                    predictions.extend(probs[:, 1].cpu().numpy())\n",
    "            \n",
    "            return np.array(predictions[:len(texts)])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during DeBERTa prediction: {e}\")\n",
    "            return np.full(len(texts), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3319d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.090867Z",
     "iopub.status.busy": "2025-07-18T15:57:25.090650Z",
     "iopub.status.idle": "2025-07-18T15:57:25.102195Z",
     "shell.execute_reply": "2025-07-18T15:57:25.101518Z"
    },
    "papermill": {
     "duration": 0.015621,
     "end_time": "2025-07-18T15:57:25.103247",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.087626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ZeroShotInference:\n",
    "    \"\"\"Load and run inference with trained zero-shot model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.cache = {}\n",
    "        self.config = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the saved zero-shot model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading zero-shot model from {self.model_path}\")\n",
    "            \n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            \n",
    "            # Load model\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_path)\n",
    "            self.model = self.model.to(InferenceConfig.DEVICE)\n",
    "            self.model.eval()\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load cache and config\n",
    "            info_path = os.path.join(self.model_path, 'zero_shot_info.pkl')\n",
    "            if os.path.exists(info_path):\n",
    "                with open(info_path, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                    self.cache = data.get('cache', {})\n",
    "                    self.config = data.get('config', {})\n",
    "            \n",
    "            print(\"Zero-shot model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading zero-shot model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_log_likelihood(self, text):\n",
    "        \"\"\"Calculate log-likelihood with caching\"\"\"\n",
    "        if text in self.cache:\n",
    "            return self.cache[text]\n",
    "        \n",
    "        try:\n",
    "            if len(text.split()) > 150:\n",
    "                text = ' '.join(text.split()[:150])\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=InferenceConfig.MAX_LENGTH,\n",
    "                padding=True\n",
    "            ).to(InferenceConfig.DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, labels=inputs['input_ids'])\n",
    "                log_likelihood = -outputs.loss.item()\n",
    "            \n",
    "            self.cache[text] = log_likelihood\n",
    "            return log_likelihood\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 0.0\n",
    "    \n",
    "    def perturb_text(self, text, num_perturbations=10):\n",
    "        \"\"\"Simple text perturbation for inference\"\"\"\n",
    "        perturbations = []\n",
    "        tokens = text.split()\n",
    "        \n",
    "        if len(tokens) < 3:\n",
    "            return [text]\n",
    "        \n",
    "        for _ in range(num_perturbations):\n",
    "            try:\n",
    "                perturbed_tokens = tokens.copy()\n",
    "                \n",
    "                # Random word dropout\n",
    "                if len(perturbed_tokens) > 3 and np.random.random() > 0.5:\n",
    "                    drop_idx = np.random.randint(0, len(perturbed_tokens))\n",
    "                    perturbed_tokens.pop(drop_idx)\n",
    "                \n",
    "                perturbed_text = ' '.join(perturbed_tokens)\n",
    "                if perturbed_text != text and len(perturbed_text.strip()) > 0:\n",
    "                    perturbations.append(perturbed_text)\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return perturbations if perturbations else [text]\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"Make zero-shot predictions\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        \n",
    "        print(f\"Making zero-shot predictions on {len(texts)} samples...\")\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Processing text {i+1}/{len(texts)}\")\n",
    "            \n",
    "            try:\n",
    "                # Original log-likelihood\n",
    "                original_ll = self.get_log_likelihood(text)\n",
    "                \n",
    "                # Perturbed log-likelihoods\n",
    "                perturbations = self.perturb_text(text)\n",
    "                perturbed_lls = [self.get_log_likelihood(p) for p in perturbations]\n",
    "                \n",
    "                if perturbed_lls:\n",
    "                    mean_perturbed_ll = np.mean(perturbed_lls)\n",
    "                    std_perturbed_ll = np.std(perturbed_lls) if len(perturbed_lls) > 1 else 1.0\n",
    "                    \n",
    "                    curvature_score = (original_ll - mean_perturbed_ll) / (std_perturbed_ll + 1e-8)\n",
    "                    prob = 1 / (1 + np.exp(-curvature_score * 2))\n",
    "                    predictions.append(prob)\n",
    "                else:\n",
    "                    predictions.append(0.5)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {i}: {e}\")\n",
    "                predictions.append(0.5)\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a8f87d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.108821Z",
     "iopub.status.busy": "2025-07-18T15:57:25.108564Z",
     "iopub.status.idle": "2025-07-18T15:57:25.116274Z",
     "shell.execute_reply": "2025-07-18T15:57:25.115589Z"
    },
    "papermill": {
     "duration": 0.011852,
     "end_time": "2025-07-18T15:57:25.117376",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.105524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFIDFInference:\n",
    "    \"\"\"Load and run inference with trained TF-IDF model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.vectorizers = None\n",
    "        self.classifiers = None\n",
    "        self.feature_extractor = None\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the saved TF-IDF model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading TF-IDF model from {self.model_path}\")\n",
    "            \n",
    "            model_file = os.path.join(self.model_path, 'tfidf_model.pkl')\n",
    "            with open(model_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                \n",
    "            self.vectorizers = data['vectorizers']\n",
    "            self.classifiers = data['classifiers']\n",
    "            self.feature_extractor = data['feature_extractor']\n",
    "            self.is_loaded = data['is_fitted']\n",
    "            \n",
    "            print(\"TF-IDF model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading TF-IDF model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"Make TF-IDF predictions\"\"\"\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "        \n",
    "        print(f\"Making TF-IDF predictions on {len(texts)} samples...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract features\n",
    "            all_features = []\n",
    "            for vectorizer in self.vectorizers:\n",
    "                features = vectorizer.transform(texts)\n",
    "                all_features.append(features)\n",
    "            \n",
    "            stat_features = self.feature_extractor.extract_features(texts)\n",
    "            \n",
    "            # Get predictions from all classifiers\n",
    "            all_predictions = []\n",
    "            for i, classifier in enumerate(self.classifiers):\n",
    "                if i < len(all_features):\n",
    "                    combined_features = np.hstack([\n",
    "                        all_features[i].toarray(),\n",
    "                        stat_features.values\n",
    "                    ])\n",
    "                else:\n",
    "                    combined_tfidf = np.hstack([f.toarray() for f in all_features])\n",
    "                    combined_features = np.hstack([\n",
    "                        combined_tfidf,\n",
    "                        stat_features.values\n",
    "                    ])\n",
    "                \n",
    "                if hasattr(classifier, 'predict_proba'):\n",
    "                    pred = classifier.predict_proba(combined_features)[:, 1]\n",
    "                else:\n",
    "                    pred = classifier.predict(combined_features).astype(float)\n",
    "                \n",
    "                all_predictions.append(pred)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            weights = [0.4, 0.3, 0.3]\n",
    "            final_prediction = np.average(all_predictions, axis=0, weights=weights)\n",
    "            \n",
    "            return final_prediction\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in TF-IDF prediction: {e}\")\n",
    "            return np.full(len(texts), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d73897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.122890Z",
     "iopub.status.busy": "2025-07-18T15:57:25.122657Z",
     "iopub.status.idle": "2025-07-18T15:57:25.130635Z",
     "shell.execute_reply": "2025-07-18T15:57:25.129996Z"
    },
    "papermill": {
     "duration": 0.01203,
     "end_time": "2025-07-18T15:57:25.131777",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.119747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnsembleInference:\n",
    "    \"\"\"Load and run inference with trained ensemble model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.meta_classifiers = None\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the saved ensemble model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading ensemble model from {self.model_path}\")\n",
    "            \n",
    "            model_file = os.path.join(self.model_path, 'ensemble_model.pkl')\n",
    "            with open(model_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                \n",
    "            self.meta_classifiers = data['meta_classifiers']\n",
    "            self.is_loaded = data['is_fitted']\n",
    "            \n",
    "            print(\"Ensemble model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ensemble model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_advanced_features(self, component_predictions):\n",
    "        \"\"\"Create advanced features from component predictions\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Original predictions\n",
    "        features.extend(component_predictions)\n",
    "        \n",
    "        # Interaction features\n",
    "        for i in range(len(component_predictions)):\n",
    "            for j in range(i+1, len(component_predictions)):\n",
    "                features.append(component_predictions[i] * component_predictions[j])\n",
    "        \n",
    "        # Statistical features\n",
    "        stacked = np.column_stack(component_predictions)\n",
    "        features.append(np.mean(stacked, axis=1))\n",
    "        features.append(np.std(stacked, axis=1))\n",
    "        features.append(np.max(stacked, axis=1))\n",
    "        features.append(np.min(stacked, axis=1))\n",
    "        \n",
    "        # Confidence features\n",
    "        for pred in component_predictions:\n",
    "            confidence = np.abs(pred - 0.5)\n",
    "            features.append(confidence)\n",
    "        \n",
    "        return np.column_stack(features)\n",
    "    \n",
    "    def predict(self, component_predictions):\n",
    "        \"\"\"Make ensemble predictions\"\"\"\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "        \n",
    "        try:\n",
    "            # Create advanced features\n",
    "            advanced_features = self.create_advanced_features(component_predictions)\n",
    "            \n",
    "            # Get predictions from all meta-classifiers\n",
    "            meta_predictions = []\n",
    "            for meta_classifier in self.meta_classifiers:\n",
    "                if hasattr(meta_classifier, 'predict_proba'):\n",
    "                    pred = meta_classifier.predict_proba(advanced_features)[:, 1]\n",
    "                else:\n",
    "                    pred = meta_classifier.predict(advanced_features).astype(float)\n",
    "                meta_predictions.append(pred)\n",
    "            \n",
    "            # Final ensemble\n",
    "            final_prediction = np.mean(meta_predictions, axis=0)\n",
    "            \n",
    "            return final_prediction\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ensemble prediction: {e}\")\n",
    "            return np.mean(component_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a23c3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.137359Z",
     "iopub.status.busy": "2025-07-18T15:57:25.137162Z",
     "iopub.status.idle": "2025-07-18T15:57:25.145105Z",
     "shell.execute_reply": "2025-07-18T15:57:25.144584Z"
    },
    "papermill": {
     "duration": 0.01196,
     "end_time": "2025-07-18T15:57:25.146117",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.134157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StatisticalFeatureExtractor:\n",
    "    \"\"\"Statistical feature extraction for inference\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def extract_features(self, texts):\n",
    "        \"\"\"Extract statistical features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "            try:\n",
    "                text_features = {}\n",
    "                \n",
    "                # Basic statistics\n",
    "                text_features['length'] = len(text)\n",
    "                words = text.split()\n",
    "                text_features['word_count'] = len(words)\n",
    "                sentences = re.split(r'[.!?]+', text)\n",
    "                text_features['sentence_count'] = len([s for s in sentences if s.strip()])\n",
    "                \n",
    "                # Word-level statistics\n",
    "                if words:\n",
    "                    text_features['avg_word_length'] = np.mean([len(word) for word in words])\n",
    "                    text_features['max_word_length'] = max(len(word) for word in words)\n",
    "                    text_features['unique_word_ratio'] = len(set(words)) / len(words)\n",
    "                else:\n",
    "                    text_features['avg_word_length'] = 0\n",
    "                    text_features['max_word_length'] = 0\n",
    "                    text_features['unique_word_ratio'] = 0\n",
    "                \n",
    "                # Character statistics\n",
    "                text_features['punct_ratio'] = len(re.findall(r'[^\\w\\s]', text)) / max(len(text), 1)\n",
    "                text_features['caps_ratio'] = len(re.findall(r'[A-Z]', text)) / max(len(text), 1)\n",
    "                text_features['digit_ratio'] = len(re.findall(r'\\d', text)) / max(len(text), 1)\n",
    "                \n",
    "                # Sentence statistics\n",
    "                if text_features['sentence_count'] > 0:\n",
    "                    text_features['avg_sentence_length'] = text_features['word_count'] / text_features['sentence_count']\n",
    "                else:\n",
    "                    text_features['avg_sentence_length'] = 0\n",
    "                \n",
    "                # Additional features\n",
    "                text_features['question_count'] = text.count('?')\n",
    "                text_features['exclamation_count'] = text.count('!')\n",
    "                text_features['comma_count'] = text.count(',')\n",
    "                \n",
    "                complex_words = [word for word in words if len(word) > 6]\n",
    "                text_features['complex_word_ratio'] = len(complex_words) / max(len(words), 1)\n",
    "                \n",
    "                features.append(text_features)\n",
    "                \n",
    "            except Exception as e:\n",
    "                features.append({\n",
    "                    'length': 0, 'word_count': 0, 'sentence_count': 0,\n",
    "                    'avg_word_length': 0, 'max_word_length': 0, 'unique_word_ratio': 0,\n",
    "                    'punct_ratio': 0, 'caps_ratio': 0, 'digit_ratio': 0,\n",
    "                    'avg_sentence_length': 0, 'question_count': 0,\n",
    "                    'exclamation_count': 0, 'comma_count': 0, 'complex_word_ratio': 0\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(features)\n",
    "        self.feature_names = df.columns.tolist()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6f3b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.151615Z",
     "iopub.status.busy": "2025-07-18T15:57:25.151407Z",
     "iopub.status.idle": "2025-07-18T15:57:25.164653Z",
     "shell.execute_reply": "2025-07-18T15:57:25.164153Z"
    },
    "papermill": {
     "duration": 0.017356,
     "end_time": "2025-07-18T15:57:25.165824",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.148468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceSolution:\n",
    "    \"\"\"Main inference solution class\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.deberta_inference = None\n",
    "        self.zero_shot_inference = None\n",
    "        self.tfidf_inference = None\n",
    "        self.ensemble_inference = None\n",
    "        \n",
    "    def load_all_models(self):\n",
    "        \"\"\"Load all trained models\"\"\"\n",
    "        print(\"Loading all trained models...\")\n",
    "        \n",
    "        try:\n",
    "            # Load DeBERTa\n",
    "            deberta_path = os.path.join(InferenceConfig.MODEL_SAVE_DIR, \"deberta\")\n",
    "            if os.path.exists(deberta_path):\n",
    "                self.deberta_inference = DeBERTaInference(deberta_path)\n",
    "            \n",
    "            # Load Zero-shot\n",
    "            zero_shot_path = os.path.join(InferenceConfig.MODEL_SAVE_DIR, \"zero_shot\")\n",
    "            if os.path.exists(zero_shot_path):\n",
    "                self.zero_shot_inference = ZeroShotInference(zero_shot_path)\n",
    "            \n",
    "            # Load TF-IDF\n",
    "            tfidf_path = os.path.join(InferenceConfig.MODEL_SAVE_DIR, \"tfidf\")\n",
    "            if os.path.exists(tfidf_path):\n",
    "                self.tfidf_inference = TFIDFInference(tfidf_path)\n",
    "            \n",
    "            # Load Ensemble\n",
    "            ensemble_path = os.path.join(InferenceConfig.MODEL_SAVE_DIR, \"ensemble\")\n",
    "            if os.path.exists(ensemble_path):\n",
    "                self.ensemble_inference = EnsembleInference(ensemble_path)\n",
    "            \n",
    "            print(\"All models loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load test data\"\"\"\n",
    "        try:\n",
    "            test_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "            print(f\"Test samples: {len(test_df)}\")\n",
    "            return test_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading test data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_texts(self, texts):\n",
    "        \"\"\"Preprocess test texts\"\"\"\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            text = str(text).strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            processed_texts.append(text)\n",
    "        return processed_texts\n",
    "    \n",
    "    def make_predictions(self, test_df):\n",
    "        \"\"\"Make final predictions using all models\"\"\"\n",
    "        print(\"Making predictions on test data...\")\n",
    "        \n",
    "        try:\n",
    "            texts = self.preprocess_texts(test_df['text'].values)\n",
    "            \n",
    "            # Get predictions from each model\n",
    "            component_predictions = []\n",
    "            \n",
    "            # DeBERTa predictions\n",
    "            if self.deberta_inference:\n",
    "                print(\"Getting DeBERTa predictions...\")\n",
    "                deberta_preds = self.deberta_inference.predict(texts)\n",
    "                component_predictions.append(deberta_preds)\n",
    "            \n",
    "            # TF-IDF predictions\n",
    "            if self.tfidf_inference:\n",
    "                print(\"Getting TF-IDF predictions...\")\n",
    "                tfidf_preds = self.tfidf_inference.predict(texts)\n",
    "                component_predictions.append(tfidf_preds)\n",
    "            \n",
    "            # Zero-shot predictions (limit to reasonable number for speed)\n",
    "            if self.zero_shot_inference:\n",
    "                print(\"Getting zero-shot predictions...\")\n",
    "                zero_shot_sample_size = min(len(texts), 500)\n",
    "                zero_shot_texts = texts[:zero_shot_sample_size]\n",
    "                zero_shot_preds = self.zero_shot_inference.predict(zero_shot_texts)\n",
    "                \n",
    "                # Pad if needed\n",
    "                if len(zero_shot_preds) < len(texts):\n",
    "                    zero_shot_preds = np.pad(\n",
    "                        zero_shot_preds,\n",
    "                        (0, len(texts) - len(zero_shot_preds)),\n",
    "                        mode='constant',\n",
    "                        constant_values=0.5\n",
    "                    )\n",
    "                component_predictions.append(zero_shot_preds)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            if self.ensemble_inference and len(component_predictions) >= 2:\n",
    "                print(\"Getting ensemble predictions...\")\n",
    "                final_predictions = self.ensemble_inference.predict(component_predictions)\n",
    "            elif component_predictions:\n",
    "                # Simple average if no ensemble model\n",
    "                print(\"Using simple average of component predictions...\")\n",
    "                final_predictions = np.mean(component_predictions, axis=0)\n",
    "            else:\n",
    "                print(\"No models available, using default predictions...\")\n",
    "                final_predictions = np.full(len(texts), 0.5)\n",
    "            \n",
    "            return final_predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error making predictions: {e}\")\n",
    "            return np.full(len(test_df), 0.5)\n",
    "    \n",
    "    def create_submission(self, test_df, predictions):\n",
    "        \"\"\"Create submission file\"\"\"\n",
    "        try:\n",
    "            submission_df = pd.DataFrame({\n",
    "                'id': test_df['id'],\n",
    "                'generated': predictions\n",
    "            })\n",
    "            \n",
    "            submission_df.to_csv('submission.csv', index=False)\n",
    "            print(f\"Submission saved with {len(submission_df)} predictions\")\n",
    "            print(f\"Prediction statistics:\")\n",
    "            print(f\"  Mean: {predictions.mean():.4f}\")\n",
    "            print(f\"  Std: {predictions.std():.4f}\")\n",
    "            print(f\"  Min: {predictions.min():.4f}\")\n",
    "            print(f\"  Max: {predictions.max():.4f}\")\n",
    "            \n",
    "            return submission_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating submission: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run_inference(self):\n",
    "        \"\"\"Run complete inference pipeline\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"LLM AI Detection Competition - INFERENCE PHASE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Running on: {InferenceConfig.DEVICE}\")\n",
    "        print(f\"Model directory: {InferenceConfig.MODEL_SAVE_DIR}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Load models\n",
    "            self.load_all_models()\n",
    "            \n",
    "            # Load test data\n",
    "            test_df = self.load_test_data()\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = self.make_predictions(test_df)\n",
    "            \n",
    "            # Create submission\n",
    "            submission_df = self.create_submission(test_df, predictions)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"INFERENCE COMPLETED!\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Total time: {total_time:.2f}s\")\n",
    "            print(f\"Processed {len(test_df)} samples\")\n",
    "            print(\"Submission file: submission.csv\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            return submission_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in inference: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227335b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T15:57:25.171254Z",
     "iopub.status.busy": "2025-07-18T15:57:25.171059Z",
     "iopub.status.idle": "2025-07-18T15:58:01.386615Z",
     "shell.execute_reply": "2025-07-18T15:58:01.385663Z"
    },
    "papermill": {
     "duration": 36.219627,
     "end_time": "2025-07-18T15:58:01.387969",
     "exception": false,
     "start_time": "2025-07-18T15:57:25.168342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LLM AI Detection Competition - INFERENCE PHASE\n",
      "============================================================\n",
      "Running on: cuda\n",
      "Model directory: /kaggle/input/k/modelbuilderpro/llm-detect-ai-generated-text/saved_models/\n",
      "============================================================\n",
      "Loading all trained models...\n",
      "All models loaded successfully\n",
      "Test samples: 3\n",
      "Making predictions on test data...\n",
      "Getting DeBERTa predictions...\n",
      "Loading DeBERTa model from /kaggle/input/k/modelbuilderpro/llm-detect-ai-generated-text/saved_models/deberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 15:57:33.093600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752854253.268136      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752854253.326558      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained for 12 epochs\n",
      "DeBERTa model loaded successfully\n",
      "Making DeBERTa predictions on 3 samples...\n",
      "Getting TF-IDF predictions...\n",
      "Loading TF-IDF model from /kaggle/input/k/modelbuilderpro/llm-detect-ai-generated-text/saved_models/tfidf\n",
      "TF-IDF model loaded successfully\n",
      "Making TF-IDF predictions on 3 samples...\n",
      "Getting zero-shot predictions...\n",
      "Loading zero-shot model from /kaggle/input/k/modelbuilderpro/llm-detect-ai-generated-text/saved_models/zero_shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot model loaded successfully\n",
      "Making zero-shot predictions on 3 samples...\n",
      "Processing text 1/3\n",
      "Getting ensemble predictions...\n",
      "Loading ensemble model from /kaggle/input/k/modelbuilderpro/llm-detect-ai-generated-text/saved_models/ensemble\n",
      "Ensemble model loaded successfully\n",
      "Submission saved with 3 predictions\n",
      "Prediction statistics:\n",
      "  Mean: 0.2943\n",
      "  Std: 0.0003\n",
      "  Min: 0.2940\n",
      "  Max: 0.2947\n",
      "============================================================\n",
      "INFERENCE COMPLETED!\n",
      "============================================================\n",
      "Total time: 36.21s\n",
      "Processed 3 samples\n",
      "Submission file: submission.csv\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "INFERENCE PHASE COMPLETED!\n",
      "Submission file created successfully.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Main execution - INFERENCE ONLY\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run inference\n",
    "    solution = InferenceSolution()\n",
    "    submission = solution.run_inference()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE PHASE COMPLETED!\")\n",
    "    print(\"Submission file created successfully.\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "sourceId": 251139752,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 53.21851,
   "end_time": "2025-07-18T15:58:04.709158",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-18T15:57:11.490648",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
